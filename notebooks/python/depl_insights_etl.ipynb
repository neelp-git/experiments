{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Insights ETL and SQL\n",
    "\n",
    "ETL for the Deployment Insights database with sample SQL queries using Trino and the Aerospike Trino Connector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that the Aerospike Database is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!asd >& /dev/null\n",
    "!pgrep -x asd >/dev/null && echo \"Aerospike database is running!\" || echo \"**Aerospike database is not running!**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Client\n",
    "Initialize Python Client used to access features stored in the Aerospike feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aerospike\n",
    "import sys\n",
    "# connect to the database\n",
    "config = {\n",
    "  'hosts': [ ('127.0.0.1', 3000) ]\n",
    "}\n",
    "try:\n",
    "  client = aerospike.client(config).connect('pensive','p3n$1v3')\n",
    "except:\n",
    "  print(\"failed to connect to the cluster with\", config['hosts'])\n",
    "  sys.exit(1)\n",
    "print('Client initialized and connected to database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Insights from Collect-Info Documents\n",
    "A Support case has attachments that are produced by the collect-info tool. The tgz archive of the attachments consists of many files, of which we use the following to extract insights:\n",
    "- [timestamp]_ascinfo.json\n",
    "- [timestamp]_ascollectinfo.log\n",
    "\n",
    "In the future we will also use this source from the archive:\n",
    "- [timestamp]_summary.log\n",
    "\n",
    "Extract the case tgz archives that you want to process, and upload the above files into a directory. Set `DATA_DIR` to this directory below. The notebook uses one json file as an example to illistrate the ETL process in the first part. Pick one json file and assign it to `EXAMPLE_JSON_FILE`.\n",
    "\n",
    "In the later part, all the files in the `DATA_DIR` directory are batch processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data'\n",
    "EXAMPLE_JSON_FILE = '20220115_004012_ascinfo.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def readJsonDataFromFile(jsonFile):  \n",
    "    # JSON file\n",
    "    f = open (jsonFile, \"r\")\n",
    "    # Reading from file\n",
    "    data = json.loads(f.read())\n",
    "    # Closing file\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "json_file =  DATA_DIR + '/' + EXAMPLE_JSON_FILE    #example json file path\n",
    "ascinfo_json = readJsonDataFromFile(json_file)\n",
    "print('Read ascinfo json file:', json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top level info: timestamp and cluster\n",
    "def get_timestamp(json):\n",
    "    timestamp = list(json.keys())[0]\n",
    "    return timestamp\n",
    "\n",
    "def get_cluster_items(json):\n",
    "    ts_items = list(json.items())\n",
    "    cluster_items = list(ts_items[0][1].items())\n",
    "    return cluster_items\n",
    "\n",
    "def get_node_items(cluster_items):\n",
    "    return list(cluster_items[0][1].items())\n",
    "\n",
    "timestamp = get_timestamp(ascinfo_json)\n",
    "print('timestamp:', timestamp)\n",
    "\n",
    "cluster_itmes = get_cluster_items(ascinfo_json)\n",
    "cluster_name = cluster_itmes[0][0]\n",
    "print('cluster name:', cluster_name)\n",
    "\n",
    "node_items = get_node_items(cluster_itmes)\n",
    "num_nodes = len(node_items)\n",
    "print('number of nodes:', num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cluster_itmes[0][1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Insights\n",
    "Collect the following cluster insights:\n",
    "\n",
    "- name\n",
    "- timestamp\n",
    "- num_nodes \n",
    "- num_ns\n",
    "- features: List (xdr, strong_consistency, single_bin, data_in_index, ...)\n",
    "- num_objects\n",
    "- storage_engines\n",
    "- num_device_bytes\n",
    "- num_memory bytes\n",
    "\n",
    "Collect the following namespace insights:\n",
    "- name\n",
    "- strong_consistency\n",
    "- num_secondary_indices\n",
    "- num_sets\n",
    "- num_bins\n",
    "- num_device_bytes\n",
    "- num_memory_bytes\n",
    "- num_objects\n",
    "- replication_factor\n",
    "- single_bin (true/false)\n",
    "- data_in_index (true/false)\n",
    "- storage_engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespace Info\n",
    "\n",
    "Collect features for each distinct namespace from each node and aggregate stats (device/mem_bytes, objects, storage_engines):\n",
    "```\n",
    "for each node:\n",
    "    for each namespace:\n",
    "        if new, \n",
    "            add namespace to namespaces map\n",
    "            copy features\n",
    "        # else: ensure settings are same\n",
    "        Aggregate stats      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_namespace_info(node_items):\n",
    "    # Potential future additions:\n",
    "                #Secondary index #, types?\n",
    "                #Total device bytes + total memory bytes - aggregate from sets    namespace_info = {}\n",
    "    namespace_info = {}\n",
    "    for node, node_subtree in node_items:\n",
    "        for ns, ns_subtree in node_subtree['as_stat']['statistics']['namespace'].items():\n",
    "            if ns not in namespace_info:\n",
    "                nsinfo = {}\n",
    "                nsinfo['name'] = ns\n",
    "                nsinfo['num_bins'] = ns_subtree['bin'].get('bin_names', 0) \n",
    "                nsinfo['num_sets'] = len(ns_subtree['set'])\n",
    "                nsinfo['num_sindex'] = len(ns_subtree['sindex'])\n",
    "                nsinfo['replication_factor'] = ns_subtree['service'].get('replication-factor', 0) \n",
    "                nsinfo['storage_engine'] = ns_subtree['service']['storage-engine']\n",
    "                nsinfo['ns_cluster_size'] = ns_subtree['service'].get('ns_cluster_size', 0)\n",
    "                nsinfo['master_objects'] = ns_subtree['service']['master_objects']\n",
    "                nsinfo['objects'] = ns_subtree['service']['objects']\n",
    "                nsinfo['single_bin'] = ns_subtree['service']['single-bin']\n",
    "                nsinfo['strong_consistency'] = ns_subtree['service'].get('strong-consistency', 'false')\n",
    "                nsinfo['data_in_index'] = ns_subtree['service']['data-in-index']\n",
    "                namespace_info[ns] = nsinfo\n",
    "    return namespace_info\n",
    "\n",
    "namespace_info = get_namespace_info(node_items)\n",
    "print(namespace_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Info\n",
    "\n",
    "Collect features at the cluster level:\n",
    "\n",
    "- Case, customer, timestamp, num_nodes\n",
    "- edition, asd-build\n",
    "- Aggregated from namespaces: num_ns, features, device/mem bytes, objects, storage engines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_info(timestamp, cluster_name, node_items, namespace_info):\n",
    "    cluster_info = {}\n",
    "    cluster_info['timestamp'] = timestamp\n",
    "    cluster_info['cluster_name'] = cluster_name \n",
    "    cluster_info['cluster_size'] = len(node_items)\n",
    "    #cluster_info['edition'] = node_items[0][1]['as_stat']['meta_data'].get('edition', 'unspecified')\n",
    "    cluster_info['server_release'] = node_items[0][1]['as_stat']['meta_data']['asd_build']\n",
    "    #cluster_info['num_namespaces'] = len(namespace_info)\n",
    "\n",
    "    cluster_info['total_objects'] = 0\n",
    "    cluster_info['storage_engines'] = set()\n",
    "    for nsinfo in namespace_info.values():\n",
    "        cluster_info['total_objects'] = cluster_info['total_objects'] + int(nsinfo['objects']) \n",
    "        cluster_info['storage_engines'] |= set([nsinfo['storage_engine']]) \n",
    "        #cluster_info['features_in_use'] |= set(['single_bin'] if nsinfo['single_bin'] == 'true' else [])\n",
    "        #cluster_info['features_in_use'] |= set(['strong_consistency'] if nsinfo['strong_consistency'] == 'true' else [])\n",
    "        #cluster_info['features_in_use'] |= set(['data_in_index'] if nsinfo['data_in_index'] == 'true' else [])\n",
    "    cluster_info['storage_engines'] = list(cluster_info['storage_engines'])\n",
    "    return cluster_info\n",
    "\n",
    "FEATURE_KEY_MAP = {\n",
    "    'AGGREGATION' : 'aggregation',\n",
    "    'BATCH' : 'batch',\n",
    "    'INDEX-ON-DEVICE' : 'index_on_device',\n",
    "    'INDEX-ON-PMEM' : 'index_on_pmem',\n",
    "    'KVS' : 'kvs',\n",
    "    'LDT' : 'ldt',\n",
    "    'QUERY' : 'query',\n",
    "    'RACK-AWARE' : 'rack_aware',\n",
    "    'SC' : 'sc',\n",
    "    'SCAN' : 'scan',\n",
    "    'SECURITY' : 'security',\n",
    "    'SINDEX' : 'sindex',\n",
    "    'TLS (FABRIC)' : 'tls_fabric',\n",
    "    'TLS (HEARTBEAT)' : 'tls_heartbeat',\n",
    "    'TLS (SERVICE)' : 'tls_service',\n",
    "    'UDF' : 'udf',\n",
    "    'XDR DESTINATION' : 'xdr_dest',\n",
    "    'XDR SOURCE' : 'xdr_src'\n",
    "}\n",
    "\n",
    "def set_features_in_use(json_path, cluster_info):\n",
    "    '''\n",
    "    for node, node_subtree in node_items:\n",
    "        cluster_info['features_in_use'] |= set(['xdr'] if len(node_subtree['as_stat']['config'].get('xdr', {})) > 0 else [])\n",
    "        # infer other features\n",
    "    cluster_info['features_in_use'] = list(cluster_info['features_in_use'])\n",
    "    '''\n",
    "    log_file = json_path[:-12] + 'ascollectinfo.log'\n",
    "    print ('ascollectinfo log file:', log_file)\n",
    "    # sed -n '/Features/,/ASCOLLECTINFO/p' 20220115_004012_ascollectinfo.log | grep YES | sed 's/[ \\t]*:.*//'\n",
    "    import os\n",
    "    stream = os.popen(\"sed -n '/Features/,/ASCOLLECTINFO/p' \" + log_file +  \" | grep ' YES' | sed 's/[ \\t]*:.*//'\")\n",
    "    features = stream.read().strip().split('\\n')\n",
    "    print('features read:', features)\n",
    "    cluster_info['features'] = set()  \n",
    "    for feature in features:\n",
    "        if feature not in FEATURE_KEY_MAP:\n",
    "            continue\n",
    "        cluster_info['features'] |= set([FEATURE_KEY_MAP[feature]])\n",
    "    cluster_info['features'] = list(cluster_info['features'])\n",
    "    return cluster_info\n",
    "    \n",
    "cluster_info = get_cluster_info(timestamp, cluster_name, node_items, namespace_info)\n",
    "cluster_info = set_features_in_use(json_file, cluster_info)\n",
    "print('cluster info:', cluster_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Into Database\n",
    "Insert the case record with case-number as the (user) key, and bins: timestamp, customer, cluster, and namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE = 'test'\n",
    "SET = 'insights'\n",
    "def load(case_num, customer, timestamp, cluster_info, namespace_info):\n",
    "    client.put((NAMESPACE, SET, case_num), \n",
    "               {'case_num': case_num,\n",
    "                'customer': customer,\n",
    "                **cluster_info,\n",
    "               'namespaces':list(namespace_info.values())})\n",
    "    return\n",
    "\n",
    "CASE_NUM = 100\n",
    "CUST_NAME = 'Widgets, Inc.'\n",
    "load(CASE_NUM, CUST_NAME, timestamp, cluster_info, namespace_info)\n",
    "print('record inserted into the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Data in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aql -c \"set output raw; select * from test.insights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch ETL: Process All Files\n",
    "Now we ETL all ascinfo.json and ascollectinfo.log files in the `DATA_DIR` directory.\n",
    "\n",
    "We assign fictious case numbers and customer names for now to the data (later to be obtained from the Support database). We sequentially assign case numbers starting at `CASE_NUM`, and rotate customer names from the `CUST_NAMES` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE_NUM_START = 200\n",
    "CUST_NAMES = ['Widgets, Inc','Wares Corp','Parts Ltd','Component Factory','Modular Design','We Assemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def process_batch_etl():\n",
    "    case_num = CASE_NUM_START\n",
    "    for json_file in glob.glob(DATA_DIR + '/*_ascinfo.json'):\n",
    "        json_info = readJsonDataFromFile(json_file)\n",
    "        print('Read ascinfo json file:', json_file)\n",
    "        \n",
    "        timestamp = get_timestamp(json_info)\n",
    "        print('timestamp:', timestamp)\n",
    "\n",
    "        cluster_itmes = get_cluster_items(json_info)\n",
    "        cluster_name = cluster_itmes[0][0]\n",
    "        print('cluster name:', cluster_name)\n",
    "\n",
    "        node_items = get_node_items(cluster_itmes)\n",
    "        num_nodes = len(node_items)\n",
    "        print('number of nodes:', num_nodes)\n",
    "\n",
    "        namespace_info = get_namespace_info(node_items)\n",
    "        print(namespace_info)\n",
    "        \n",
    "        cluster_info = get_cluster_info(timestamp, cluster_name, node_items, namespace_info)\n",
    "        cluster_info = set_features_in_use(json_file, cluster_info)\n",
    "        print(cluster_info)\n",
    "\n",
    "        cust_name = CUST_NAMES[case_num % len(CUST_NAMES)]\n",
    "        load(case_num, cust_name, timestamp, cluster_info, namespace_info)\n",
    "        case_num += 1\n",
    "        print('record inserted into the database')        \n",
    "        \n",
    "        print('\\n')\n",
    "    return\n",
    "    \n",
    "process_batch_etl()\n",
    "print('batch etl done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aql -c \"set output raw; select * from test.insights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trino SQL Queries\n",
    "For the following queries to exexute in the notebook, you must have a Trino server running at port 8080 of the host, connected to this container's Aerospike database via the Aerospike Trino Connector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trino Setup\n",
    "1. Make sure the port of the Aerospike server (typically 3000) running in this notebook's container is exposed to host. \n",
    "2. [Use these instruactions](https://github.com/citrusleaf/aerospike-connect-trino#run-on-docker) to run Trino and the Aerospike Trino Connector in a docker container on the host machine. Make sure the Trino Connector can access the Aerospike server in this container.\n",
    "3. [Install the Trino Client](https://trino.io/docs/current/installation/cli.html) in this container. The following cells assume it is installed in the parent directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trino Command\n",
    "Define the environment variable for short form of the Trino command. \n",
    "\n",
    "You can also run the Trino command line tool in a separate shell tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRINO=../trino --server host.docker.internal:8080 --catalog aerospike --schema test --output-format=TSV_HEADER\n",
    "%env TRINO_VERTICAL=../trino --server host.docker.internal:8080 --catalog aerospike --schema test --output-format=VERTICAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show schemas (namespaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"show schemas\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show tables (sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"show tables\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a sample record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO_VERTICAL --execute \"select * from test.insights limit 1\" ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get customers using feature 'index_on_device'.\n",
    "```\n",
    "select customer, cluster_name, features \n",
    "from insights \n",
    "where contains(cast(features as array(VARCHAR)),'index_on_device')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select customer, cluster_name, features from insights where contains(cast(features as array(VARCHAR)),'index_on_device')\" ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get customers using feature 'xdr_dest' and release after 5.x\n",
    "```\n",
    "select customer, features, server_release \n",
    "from insights \n",
    "where contains(cast(features as array(VARCHAR)),'xdr_dest') and regexp_like(server_release, '^5.*')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select customer, features, server_release from insights where contains(cast(features as array(VARCHAR)),'xdr_dest') and regexp_like(server_release, '^5.*');\" ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the largest deployed cluster by each customer.\n",
    "```\n",
    "select customer, max(cluster_size) as max_cluster_size\n",
    "rom insights\n",
    "group by customer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select customer, max(cluster_size) as max_cluster_size from insights group by customer\" ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the largest namespace by objects for each customer.\n",
    "```\n",
    "select customer, max(transform(cast(namespaces as array<map<varchar, varchar>>), entry->entry['objects'])) \n",
    "                 as max_ns_objects\n",
    "from insights \n",
    "group by customer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select customer, max(transform(cast(namespaces as array<map<varchar, varchar>>), entry->entry['objects'])) as max_ns_objects from insights group by customer\" ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get customers with single-bin namespaces. Also get (name, storage, data_in_index) for each single-bin namespace.\n",
    "\n",
    "`\n",
    "select customer, case_num, single_bin_ns from (\n",
    "    select customer, transform(\n",
    "        filter(cast(namespaces as array<map<varchar,varchar>>), \n",
    "               ns->ns['single_bin']='true'),\n",
    "        ns->(ns['name'],ns['storage_engine'],ns['data_in_index']) \n",
    "        ) as single_bin_ns \n",
    "    from insights)\n",
    "where single_bin_ns != Array[]\n",
    "order by customer, case_num desc;\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO_VERTICAL --execute \"select customer, case_num, cluster_name, single_bin_ns from (select customer, case_num,  cluster_name, transform(filter(cast(namespaces as array<map<varchar,varchar>>), ns->ns['single_bin']='true'),ns->(ns['name'],ns['storage_engine'],ns['data_in_index']) ) as single_bin_ns from insights) where single_bin_ns != Array[] order by customer, case_num desc;\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get customers that have single-bin in-memory namespaces.\n",
    "\n",
    "`\n",
    "select customer, case_num, cluster_name, single_bin_mem_ns from (\n",
    "    select customer,  case_num, cluster_name, \n",
    "      filter(\n",
    "        transform(cast(namespaces as array<map<varchar,varchar>>), \n",
    "                  ns->map_filter(ns,(k,v)->k in \n",
    "                  ('name','single_bin','storage_engine','data_in_index','enable_xdr'))), \n",
    "                   ns->ns['single_bin']='true' and (ns['storage_engine'] = 'memory' \n",
    "                                                    or ns['data_in_index'] = 'true'))\n",
    "        as single_bin_mem_ns \n",
    "    from insights) \n",
    "where single_bin_mem_ns != Array[]\n",
    "order by customer, case_num desc;\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select customer, case_num, cluster_name, single_bin_mem_ns from (select customer, case_num, cluster_name, filter(transform(cast(namespaces as array<map<varchar,varchar>>), ns->map_filter(ns,(k,v)->k in ('name','single_bin','storage_engine','data_in_index','enable_xdr'))), ns->ns['single_bin']='true' and (ns['storage_engine'] = 'memory' or  ns['data_in_index'] = 'true')) as single_bin_mem_ns from insights) where single_bin_mem_ns != Array[] order by customer, case_num desc\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get customers that have single-bin in-memory namespaces, with xdr_src flag to indicate use of XDR.\n",
    "\n",
    "`\n",
    "select customer, case_num, cluster_name, xdr_src, single_bin_mem_ns from (\n",
    "    select customer, case_num, cluster_name, contains(cast(features as array<varchar>), \n",
    "                                                     'xdr_src') as xdr_src, \n",
    "      filter(\n",
    "        transform(cast(namespaces as array<map<varchar,varchar>>), \n",
    "                  ns->map_filter(ns,(k,v)->k in \n",
    "                  ('name','single_bin','storage_engine','enable_xdr','data_in_index))), \n",
    "                    ns->ns['single_bin']='true' and (ns['storage_engine'] = 'memory' or \n",
    "                                                     ns['data_in_index'] = 'true' ))\n",
    "        as single_bin_mem_ns \n",
    "    from insights) \n",
    "where single_bin_mem_ns != Array[]\n",
    "order by customer, case_num desc;\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select case_num, customer, cluster_name, xdr_src, single_bin_ns from (select case_num, customer, cluster_name, contains(cast(features as array<varchar>), 'xdr_src') as xdr_src, filter(transform(cast(namespaces as array<map<varchar,varchar>>), ns->map_filter(ns,(k,v)->k in ('name','single_bin','storage_engine','enable_xdr','data_in_index'))), ns->ns['single_bin']='true' and (ns['storage_engine'] = 'memory' or ns['data_in_index'] = 'true')) as single_bin_ns from insights) where single_bin_ns != Array[] order by customer, case_num desc\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select customer, cluster_name, max(timestamp) as last, features from insights group by customer, cluster_name, timestamp, features order by customer, cluster_name;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$TRINO --execute \"select __key, case_num, namespaces from insights  order by case_num;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatic Access via Trino\n",
    "Install the trino package if not already installed to enable Python access by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install trino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "conn = trino.dbapi.connect(\n",
    "    host='host.docker.internal',\n",
    "    port=8080,\n",
    "    user='admin',\n",
    "    catalog='aerospike',\n",
    "    schema='test'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute('select * from insights limit 1')\n",
    "rows = cur.fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "Run the following cell to truncate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aql -c \"truncate test.insights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
